{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1v4d9PFTzCw"
      },
      "source": [
        "# Linear Least Squares and the expansion of the universe!\n",
        "\n",
        "*************\n",
        "<img src=\"https://cdn.theatlantic.com/assets/media/img/2018/03/lead_large-1/lead_720_405.jpg?mod=1533692228\" width=\"100\" />Einstein's self-labelled \"biggest blunder\" came when he added a fudge factor that he called the *cosmological constant* to his calculations. He added it because based on his own theory of General Relativity, the universe was expanding... but that couldn't be! So he made it go away with this magical constant (OOPS!).\n",
        "\n",
        "Enter astronomer Edwin Hubble : <img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/1/15/Studio_portrait_photograph_of_Edwin_Powell_Hubble_%28cropped%29.JPG/220px-Studio_portrait_photograph_of_Edwin_Powell_Hubble_%28cropped%29.JPG\" width=\"70\" />. He observed that on average, everything was moving away from us! In fact, the farther away from us the object (such as a star or galaxy) was, the faster it was moving from us! Einstein realized his original work was right all along, and felt silly for faking math.\n",
        "\n",
        "Hubble saw a *linear* relationship, thus Hubble's law is written as: $$v = H_0 d$$\n",
        "In this activity we will measure Hubble's constant, $H_0$ using his **actual data** (so cool, right?).\n",
        "************\n",
        "\n",
        "**tl;dr**\n",
        "We are doing a linear least squares regression. The first columm in the file `hubble.csv` contains our *inputs* and the second our *outputs* to the linear model.\n",
        "\n",
        "*Note: We will use `pandas` for easy data management, `numpy` for vectorized mathematics, and `matplotlib` for plotting.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ppSFx0dTzCx"
      },
      "outputs": [],
      "source": [
        "# Package imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kycQAem7TzCy"
      },
      "outputs": [],
      "source": [
        "# Read in the data from the file hubble.csv\n",
        "\n",
        "# Check out the documentation for the read_csv() function:\n",
        "# https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
        "\n",
        "# Don't get overwhelmed by the number of possible parameters; every parameter that has an \"=\"\n",
        "# sign next to it is optional. If you leave it unspecified, it will default to some sensible\n",
        "# value. At least for this activity, you can leave them all as is, and just specify your\n",
        "# file name (i.e., call read_csv with just one argument)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beqvMlyFTzCy"
      },
      "outputs": [],
      "source": [
        "# To verify that you successfully read in the data, let's print out a small\n",
        "# section of it: the pandas head() and tail() functions are handy for this\n",
        "\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xVWFrJ0TzCy"
      },
      "outputs": [],
      "source": [
        "# The column names are a little unwieldy: modify them to something that's a little\n",
        "# easier to work with (but still descriptive!). We'll let you Google this one\n",
        "\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENdF-M_6TzCy"
      },
      "outputs": [],
      "source": [
        "# Let's plot the raw data -- d along the x-axis and v along the y-axis.\n",
        "# Here's a quick start guide: https://matplotlib.org/tutorials/introductory/pyplot.html\n",
        "# Don't forget to label your axes and title the plot!\n",
        "\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5GaUvLDTzCz"
      },
      "outputs": [],
      "source": [
        "# Let's define our hypothesis function -- no need to modify\n",
        "def y_pred(theta, D):\n",
        "    \"\"\"\n",
        "    We assume theta is an array of two parameters like [22.0, 3.5]\n",
        "        - theta[0] is the intercept term\n",
        "        - theta[1] is the slope\n",
        "    D is a vector of all the distance values from our dataset\n",
        "\n",
        "    This function will return a vector of all our predictions on the data,\n",
        "    i.e., a vector of (predicted) velocities.\n",
        "    \"\"\"\n",
        "    # the magic of vectorized code: no loops!\n",
        "    # if how this works doesn't make sense, talk to us!\n",
        "    return theta[0] + theta[1] * D"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a reminder, gradient descent is an optimization algorithm that finds the parameters $w$ that minimize a function via:\n",
        "\n",
        "$$w_j ← w_j - \\eta \\frac{∂ J}{\\partial w_j} $$\n",
        "\n",
        "Where the function we want to minimize, $J(w)$ can be defined in this problem as the mean squared error:\n",
        "\n",
        "$$J(\\vec w) = \\frac{1}{N}∑_i (y_{pred} - y_{true})^2$$"
      ],
      "metadata": {
        "id": "QDR9Ani0UAKS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1AMd69UrTzCz"
      },
      "outputs": [],
      "source": [
        "# Now for the main act: implement gradient descent to fit a line through the\n",
        "# data. This should be a 2-parameter fit, with a slope and intercept term.\n",
        "# Since the data is noisy, we'll declare convergence when the change in your\n",
        "# parameters drops below 1e-3.\n",
        "\n",
        "# One implementation tip: use vectorized code to compute the update equation.\n",
        "# You can wrap that in a loop to control how many gradient descent steps you\n",
        "# take. In other words: your solution must ultimately only contain one loop,\n",
        "# not a nested loop!\n",
        "\n",
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cwofEhBATzCz"
      },
      "outputs": [],
      "source": [
        "# Print out your final fitted parameters theta\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWQSzFz7TzCz"
      },
      "outputs": [],
      "source": [
        "# Plot your line of best fit and your data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q53zjjfTzCz"
      },
      "source": [
        "### In the cell below, comment on your results.\n",
        "\n",
        "Does your line appear to fit the data well? Hubble calculated an expansion at a rate of $H_0 \\approx 500$ km/s/Mparsec. Do your results agree?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CrvZshQTzCz"
      },
      "source": [
        "\n",
        "<_Your response goes here._>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2seIOF9TzCz"
      },
      "source": [
        "### A note on interpreting regression results\n",
        "\n",
        "Consider the following regression problem: predicting the price of a house ($\\hat{y}$) based on the number of bedrooms ($x_1$, an integer) and the total area of a house ($x_2$, measured in square feet). In other words, we are interested in fitting the following function:\n",
        "$$ \\hat{y} = \\theta_0 + \\theta_1 \\cdot x_1 + \\theta_2 \\cdot x_2 $$\n",
        "\n",
        "Once gradient descent converges, we discover that $\\theta_1 = 780.0$ and $\\theta_2 =  2.5$. On this basis, can we conclude that the variable $x_1$ (number of bedrooms) has a _far_ greater impact on a house's price than $x_2$ (area), since it has a much larger coefficient in our model? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnmHJfXCTzCz"
      },
      "source": [
        "<i><p style='text-align: right;'> <b>Authors:</b> Michelle Kuchera, Raghuram Ramanujan </p></i>"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
